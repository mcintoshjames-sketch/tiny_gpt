{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7add725e",
   "metadata": {},
   "source": [
    "# TinyGPT Training on Google Colab\n",
    "\n",
    "**Free GPU Training!** This notebook trains your TinyGPT model using Google Colab's free T4 GPU.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU (T4)\n",
    "2. **Run all cells** in order\n",
    "3. Training will take ~2-3 hours (vs 6+ hours on M4)\n",
    "4. Download trained model and tokenizer at the end\n",
    "\n",
    "**Advantages over M4:**\n",
    "- 2-3x faster training (T4 GPU vs M4 GPU)\n",
    "- Free GPU hours (no cost)\n",
    "- More VRAM (16GB vs M4's shared memory)\n",
    "- Can use larger batch sizes\n",
    "\n",
    "**âœ¨ NEW: Smart Caching**\n",
    "- First run: Encodes 539M characters (~8 minutes)\n",
    "- Subsequent runs: Loads from cache (<1 second)\n",
    "- Saves ~8 minutes on every run after the first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone your repository\n",
    "!git clone https://github.com/mcintoshjames-sketch/tiny_gpt.git\n",
    "%cd tiny_gpt\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q tokenizers datasets tqdm numpy requests\n",
    "\n",
    "print(\"\\nâœ“ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Verify GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"âœ… GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âŒ No GPU found!\")\n",
    "    print(\"   Go to: Runtime â†’ Change runtime type â†’ GPU (T4)\")\n",
    "    raise RuntimeError(\"GPU required for efficient training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Download and verify WikiText-103 dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists('data/wt103_train.txt'):\n",
    "    print(\"Downloading WikiText-103 dataset...\")\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    train_text = \"\\n\".join([t for t in dataset[\"train\"][\"text\"] if t and len(t.strip()) > 0])\n",
    "    with open('data/wt103_train.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(train_text)\n",
    "    \n",
    "    valid_text = \"\\n\".join([t for t in dataset[\"validation\"][\"text\"] if t and len(t.strip()) > 0])\n",
    "    with open('data/wt103_valid.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(valid_text)\n",
    "    \n",
    "    print(f\"âœ“ Downloaded {len(train_text):,} training characters\")\n",
    "    print(f\"âœ“ Downloaded {len(valid_text):,} validation characters\")\n",
    "else:\n",
    "    print(\"âœ“ Dataset already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15537ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the model!\n",
    "# This will take ~2-3 hours on T4 GPU\n",
    "\n",
    "!python3 train.py\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a39f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test the trained model\n",
    "!python3 inference.py --prompt \"The history of artificial intelligence\" --max_tokens 100\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "!python3 inference.py --prompt \"Machine learning is\" --max_tokens 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd805a6",
   "metadata": {},
   "source": [
    "## ðŸš€ Token Caching Feature\n",
    "\n",
    "The training script now uses **smart caching** to avoid re-encoding:\n",
    "\n",
    "- **First run**: Encodes data and saves to `data/train_tokens.npy` (~8 min)\n",
    "- **Future runs**: Loads from cache instantly (<1 sec)\n",
    "- **Manual cache clear**: Run `!rm data/train_tokens.npy data/val_tokens.npy` if you change tokenizer settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00251eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model diagnostics (after training completes or during training)\n",
    "!python3 diagnose_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Download trained model to your computer\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Preparing files for download...\\n\")\n",
    "\n",
    "# Download model checkpoint\n",
    "if os.path.exists('tiny_gpt_best.pt'):\n",
    "    print(\"Downloading tiny_gpt_best.pt...\")\n",
    "    files.download('tiny_gpt_best.pt')\n",
    "\n",
    "# Download tokenizer\n",
    "if os.path.exists('tokenizer_bpe_best.json'):\n",
    "    print(\"Downloading tokenizer_bpe_best.json...\")\n",
    "    files.download('tokenizer_bpe_best.json')\n",
    "\n",
    "print(\"\\nâœ… Downloads complete!\")\n",
    "print(\"   Place these files in your ~/tiny_gpt directory on Mac\")\n",
    "print(\"   Then run: python3 inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a14682",
   "metadata": {},
   "source": [
    "## Alternative: Mount Google Drive\n",
    "\n",
    "If you want to save directly to Google Drive (so you don't lose progress if Colab disconnects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0df149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Mount Google Drive to save checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy trained files to Google Drive\n",
    "!mkdir -p '/content/drive/MyDrive/tiny_gpt_models'\n",
    "!cp tiny_gpt_best.pt '/content/drive/MyDrive/tiny_gpt_models/'\n",
    "!cp tokenizer_bpe_best.json '/content/drive/MyDrive/tiny_gpt_models/'\n",
    "\n",
    "print(\"âœ“ Models saved to Google Drive: MyDrive/tiny_gpt_models/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
